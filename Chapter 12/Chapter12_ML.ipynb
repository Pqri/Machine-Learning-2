{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Chapter 12: Custom Models and Training with TensorFlow\n",
        "\n",
        "Chapter ini membahas cara membuat model machine learning yang benar-benar fleksibel menggunakan API low-level TensorFlow. Kita tidak hanya menggunakan Keras seperti biasanya, tetapi membuat **loss function**, **metrics**, **layers**, bahkan **training loop** secara manual.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Tujuan Utama\n",
        "\n",
        "- Memahami bagaimana cara kerja model machine learning dari bawah\n",
        "- Membuat custom loss function, custom metric, dan custom layer\n",
        "- Menulis loop pelatihan manual menggunakan `GradientTape`\n",
        "- Menggunakan `tf.data` untuk mengelola pipeline data yang efisien\n",
        "- Menyesuaikan training dengan callback buatan sendiri\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Custom Loss Function\n",
        "\n",
        "### ➕ Contoh: Huber Loss\n",
        "\n",
        "Huber Loss merupakan gabungan dari MSE dan MAE, cocok digunakan ketika data memiliki outlier.\n",
        "\n",
        "```python\n",
        "def huber_fn(y_true, y_pred):\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "CeuMKXOSriZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Amy3N6_TrA8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d83ac11-604b-48a5-a6f9-fe28751485e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ManualLoop] Epoch 1, Loss: 1.9545\n",
            "[ManualLoop] Epoch 2, Loss: 1.7886\n",
            "[ManualLoop] Epoch 3, Loss: 1.6266\n",
            "[ManualLoop] Epoch 4, Loss: 1.4690\n",
            "[ManualLoop] Epoch 5, Loss: 1.3167\n",
            "[ManualLoop] Epoch 6, Loss: 1.1704\n",
            "[ManualLoop] Epoch 7, Loss: 1.0309\n",
            "[ManualLoop] Epoch 8, Loss: 0.9002\n",
            "[ManualLoop] Epoch 9, Loss: 0.7796\n",
            "[ManualLoop] Epoch 10, Loss: 0.6704\n",
            "[tf.data] Epoch 1, Loss: 1.9246\n",
            "[tf.data] Epoch 2, Loss: 2.5335\n",
            "[tf.data] Epoch 3, Loss: 3.1604\n",
            "[tf.data] Epoch 4, Loss: 2.3626\n",
            "[tf.data] Epoch 5, Loss: 1.8503\n",
            "[tf.data] Epoch 6, Loss: 1.0719\n",
            "[tf.data] Epoch 7, Loss: 0.3387\n",
            "[tf.data] Epoch 8, Loss: 1.0078\n",
            "[tf.data] Epoch 9, Loss: 1.4268\n",
            "[tf.data] Epoch 10, Loss: 0.7995\n",
            "Epoch 1/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5626\n",
            "Epoch 2/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4664 \n",
            "Epoch 3/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4118\n",
            "Epoch 4/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3888 \n",
            "Epoch 5/5\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2407 \n",
            "Epoch 1/5\n",
            "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - loss: 1.6335Epoch 1: val/train loss ratio = 0.95\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1.6334 - val_loss: 1.5259\n",
            "Epoch 2/5\n",
            "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 1.4371Epoch 2: val/train loss ratio = 0.94\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4833 - val_loss: 1.3727\n",
            "Epoch 3/5\n",
            "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.4281Epoch 3: val/train loss ratio = 0.94\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.3946 - val_loss: 1.2293\n",
            "Epoch 4/5\n",
            "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.1546Epoch 4: val/train loss ratio = 0.93\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1934 - val_loss: 1.0969\n",
            "Epoch 5/5\n",
            "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2543Epoch 5: val/train loss ratio = 0.92\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0986 - val_loss: 0.9755\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79b228953990>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# CHAPTER 12: Custom Models and Training with TensorFlow (Gabungan Lengkap)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Loss Function: Huber Loss\n",
        "# -------------------------------------\n",
        "def huber_fn(y_true, y_pred):\n",
        "    error = y_true - y_pred\n",
        "    is_small_error = tf.abs(error) < 1.0\n",
        "    squared_loss = tf.square(error) / 2\n",
        "    linear_loss = tf.abs(error) - 0.5\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Metric Class\n",
        "# -------------------------------------\n",
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"huber_metric\", threshold=1.0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.threshold = threshold\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) < self.threshold\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = tf.abs(error) - 0.5\n",
        "        losses = tf.where(is_small_error, squared_loss, linear_loss)\n",
        "        self.total.assign_add(tf.reduce_sum(losses))\n",
        "        self.count.assign_add(tf.cast(tf.size(error), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "# -------------------------------------\n",
        "# Dummy Dataset\n",
        "# -------------------------------------\n",
        "X = tf.random.normal((1000, 1))\n",
        "y = 3 * X + 2 + tf.random.normal((1000, 1))\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Training Loop with GradientTape\n",
        "# -------------------------------------\n",
        "model = keras.Sequential([layers.Dense(1, input_shape=[1])])\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for step in range(len(X) // 32):\n",
        "        X_batch = X[step * 32: (step + 1) * 32]\n",
        "        y_batch = y[step * 32: (step + 1) * 32]\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True)\n",
        "            loss = tf.reduce_mean(huber_fn(y_batch, y_pred))\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    print(f\"[ManualLoop] Epoch {epoch + 1}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "# -------------------------------------\n",
        "# Training with tf.data Pipeline\n",
        "# -------------------------------------\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "dataset = dataset.shuffle(buffer_size=1000).batch(32).prefetch(1)\n",
        "\n",
        "model2 = keras.Sequential([layers.Dense(1, input_shape=[1])])\n",
        "optimizer2 = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for X_batch, y_batch in dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model2(X_batch, training=True)\n",
        "            loss = tf.reduce_mean(huber_fn(y_batch, y_pred))\n",
        "        grads = tape.gradient(loss, model2.trainable_variables)\n",
        "        optimizer2.apply_gradients(zip(grads, model2.trainable_variables))\n",
        "    print(f\"[tf.data] Epoch {epoch + 1}, Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Layer\n",
        "# -------------------------------------\n",
        "class MyDenseLayer(keras.layers.Layer):\n",
        "    def __init__(self, units=32, activation=None):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
        "        self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.w) + self.b)\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Model Using Custom Layer\n",
        "# -------------------------------------\n",
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden = MyDenseLayer(30, activation=\"relu\")\n",
        "        self.output_layer = MyDenseLayer(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.hidden(inputs)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "model_custom = MyModel()\n",
        "model_custom.compile(loss=huber_fn, optimizer=\"sgd\")\n",
        "model_custom.fit(X, y, epochs=5)\n",
        "\n",
        "# -------------------------------------\n",
        "# Custom Callback\n",
        "# -------------------------------------\n",
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
        "        print(f\"Epoch {epoch + 1}: val/train loss ratio = {ratio:.2f}\")\n",
        "\n",
        "# -------------------------------------\n",
        "# Model with Custom Callback\n",
        "# -------------------------------------\n",
        "model_cb = keras.Sequential([layers.Dense(1, input_shape=[1])])\n",
        "model_cb.compile(loss=huber_fn, optimizer=\"sgd\")\n",
        "model_cb.fit(X, y, epochs=5, validation_split=0.2, callbacks=[PrintValTrainRatioCallback()])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89OK0sddrWLz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
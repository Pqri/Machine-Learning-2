{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "Bab ini mengajarkan cara **memuat dan memproses data secara efisien** menggunakan `tf.data`, sistem input/output tingkat tinggi dari TensorFlow.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Tujuan Bab Ini\n",
        "- Membuat pipeline data dari array, file teks, gambar, atau TFRecord\n",
        "- Menerapkan operasi transformasi: `map`, `batch`, `shuffle`, `repeat`\n",
        "- Membangun pipeline kompleks untuk dataset besar dan beragam\n",
        "- Menormalkan, membaca, dan memproses data secara efisien\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Topik yang Dibahas\n",
        "\n",
        "1. **`from_tensor_slices()`** ‚Äì Membuat dataset dari tensor sederhana\n",
        "2. **Transformasi Data** ‚Äì `map`, `shuffle`, `batch`, `repeat`\n",
        "3. **TextLineDataset** ‚Äì Membaca file teks baris per baris\n",
        "4. **Interleave** ‚Äì Membaca banyak file secara paralel\n",
        "5. **Preprocessing Gambar** ‚Äì Baca & ubah ukuran gambar PNG\n",
        "6. **TFRecord** ‚Äì Format penyimpanan biner efisien untuk deployment\n",
        "7. **Normalisasi Custom** ‚Äì Hitung mean/std lalu normalisasi per-batch\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Kesimpulan\n",
        "\n",
        "Dengan `tf.data`, kita bisa membuat pipeline data yang:\n",
        "- Efisien dan scalable\n",
        "- Mendukung paralelisme dan prefetching\n",
        "- Cocok untuk data teks, gambar, dan format khusus (TFRecord)\n",
        "\n",
        "Pipeline yang baik = training yang cepat dan stabil!\n"
      ],
      "metadata": {
        "id": "M0fQOB_zsGX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgq9IOJbrzlC",
        "outputId": "ba71d7cf-3798-4c6f-cf6e-1885c26ca306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "# Dataset dasar (from_tensor_slices):\n",
            "0 1 2 3 4 5 6 7 8 9 \n",
            "\n",
            "# Dataset setelah map, shuffle, batch, repeat:\n",
            "[6 8 2]\n",
            "[ 4 16 10]\n",
            "[ 0 12 18]\n",
            "[14]\n",
            "[ 0  8 10]\n",
            "[12  4  6]\n",
            "[14 16  2]\n",
            "[18]\n",
            "\n",
            "\n",
            "# Membaca file baris per baris:\n",
            "line 1\n",
            "line 2\n",
            "line 3\n",
            "line 4\n",
            "\n",
            "\n",
            "# Interleave dari beberapa file:\n",
            "File 2 - Line 1\n",
            "File 1 - Line 1\n",
            "File 0 - Line 1\n",
            "File 2 - Line 2\n",
            "File 1 - Line 2\n",
            "File 0 - Line 2\n",
            "\n",
            "\n",
            "Files in image folder: ['img_1.png', 'img_2.png', 'img_0.png', 'img_4.png', 'img_3.png']\n",
            "# Batch gambar yang sudah diproses:\n",
            "Shape: (2, 28, 28, 3)\n",
            "\n",
            "\n",
            "# Membaca kembali dari TFRecord:\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "\n",
            "\n",
            "# Contoh batch yang dinormalisasi:\n",
            "Normalized batch shape: (32, 3)\n"
          ]
        }
      ],
      "source": [
        "# CHAPTER 13: Loading and Preprocessing Data with TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# =============================================================================\n",
        "# üß™ 1. Dataset Dasar (from_tensor_slices)\n",
        "# =============================================================================\n",
        "\n",
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "print(\"# Dataset dasar (from_tensor_slices):\")\n",
        "for item in dataset:\n",
        "    print(item.numpy(), end=\" \")\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üîÑ 2. Transformasi Dataset (map, shuffle, batch, repeat)\n",
        "# =============================================================================\n",
        "\n",
        "dataset = dataset.map(lambda x: x * 2)\n",
        "dataset = dataset.shuffle(buffer_size=5)\n",
        "dataset = dataset.batch(3)\n",
        "dataset = dataset.repeat(2)\n",
        "\n",
        "print(\"# Dataset setelah map, shuffle, batch, repeat:\")\n",
        "for batch in dataset:\n",
        "    print(batch.numpy())\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üìÑ 3. Membaca File Teks\n",
        "# =============================================================================\n",
        "\n",
        "file_path = \"sample_lines.txt\"\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(\"line 1\\nline 2\\nline 3\\nline 4\\n\")\n",
        "\n",
        "text_ds = tf.data.TextLineDataset(file_path)\n",
        "print(\"# Membaca file baris per baris:\")\n",
        "for line in text_ds:\n",
        "    print(line.numpy().decode(\"utf-8\"))\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üîÅ 4. Interleave File\n",
        "# =============================================================================\n",
        "\n",
        "for i in range(3):\n",
        "    with open(f\"file_{i}.txt\", \"w\") as f:\n",
        "        f.write(f\"File {i} - Line 1\\n\")\n",
        "        f.write(f\"File {i} - Line 2\\n\")\n",
        "\n",
        "file_list_ds = tf.data.Dataset.list_files(\"file_*.txt\")\n",
        "dataset = file_list_ds.interleave(\n",
        "    lambda fname: tf.data.TextLineDataset(fname),\n",
        "    cycle_length=3)\n",
        "\n",
        "print(\"# Interleave dari beberapa file:\")\n",
        "for line in dataset:\n",
        "    print(line.numpy().decode())\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üñºÔ∏è 5. Pipeline Gambar (Revisi)\n",
        "# =============================================================================\n",
        "\n",
        "img_dir = \"images\"\n",
        "os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "# Simpan gambar RGB agar tidak error saat load\n",
        "for i in range(5):\n",
        "    img = np.random.rand(28, 28, 3) * 255  # RGB\n",
        "    array_to_img(img.astype(np.uint8)).save(f\"{img_dir}/img_{i}.png\")\n",
        "\n",
        "print(\"Files in image folder:\", os.listdir(img_dir))\n",
        "\n",
        "# Load dan preprocess\n",
        "img_ds = tf.data.Dataset.list_files(f\"{img_dir}/*.png\")\n",
        "\n",
        "def load_and_preprocess(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_png(img, channels=3)\n",
        "    img = tf.image.resize(img, [28, 28])\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "img_ds = img_ds.map(load_and_preprocess).batch(2)\n",
        "\n",
        "print(\"# Batch gambar yang sudah diproses:\")\n",
        "for batch in img_ds.take(1):\n",
        "    print(\"Shape:\", batch.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# üì¶ 6. TFRecord Format\n",
        "# =============================================================================\n",
        "\n",
        "tfrecord_file = \"data.tfrecord\"\n",
        "with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "    for i in range(5):\n",
        "        feature = {\n",
        "            \"feature\": tf.train.Feature(int64_list=tf.train.Int64List(value=[i]))\n",
        "        }\n",
        "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "        writer.write(example.SerializeToString())\n",
        "\n",
        "def parse_example(serialized):\n",
        "    feature_description = {\n",
        "        \"feature\": tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    return tf.io.parse_single_example(serialized, feature_description)\n",
        "\n",
        "tfrecord_ds = tf.data.TFRecordDataset([tfrecord_file])\n",
        "tfrecord_ds = tfrecord_ds.map(parse_example)\n",
        "\n",
        "print(\"# Membaca kembali dari TFRecord:\")\n",
        "for record in tfrecord_ds:\n",
        "    print(record[\"feature\"].numpy())\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# ‚öôÔ∏è 7. Normalisasi Batch (Custom Preprocessing)\n",
        "# =============================================================================\n",
        "\n",
        "raw_data = tf.data.Dataset.from_tensor_slices(tf.random.normal([1000, 3]))\n",
        "raw_data = raw_data.batch(32)\n",
        "\n",
        "def normalize_batch(batch):\n",
        "    mean = tf.reduce_mean(batch, axis=0)\n",
        "    std = tf.math.reduce_std(batch, axis=0)\n",
        "    return (batch - mean) / std\n",
        "\n",
        "norm_ds = raw_data.map(normalize_batch)\n",
        "\n",
        "print(\"# Contoh batch yang dinormalisasi:\")\n",
        "for batch in norm_ds.take(1):\n",
        "    print(\"Normalized batch shape:\", batch.shape)\n"
      ]
    }
  ]
}
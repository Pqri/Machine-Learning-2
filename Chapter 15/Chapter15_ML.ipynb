{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“˜ Chapter 15: Processing Sequences Using RNNs and CNNs\n",
        "\n",
        "Pada bab ini, kita mempelajari bagaimana deep learning digunakan untuk memproses **data berurutan** seperti teks atau deretan angka menggunakan **Recurrent Neural Networks (RNNs)** dan **Convolutional Neural Networks (CNNs)** 1D.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Tujuan Utama\n",
        "\n",
        "- Memahami konsep dasar pemrosesan urutan (sequence processing)\n",
        "- Implementasi model RNN, LSTM, GRU, dan CNN 1D\n",
        "- Klasifikasi teks menggunakan dataset IMDB\n",
        "- Membandingkan performa berbagai arsitektur pada data sekuensial\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Dataset: IMDB Movie Reviews\n",
        "\n",
        "- Dataset terdiri dari 50.000 review film (25k train, 25k test)\n",
        "- Tugas: **klasifikasi biner** (positif atau negatif)\n",
        "- Tokenisasi dilakukan oleh Keras (`keras.datasets.imdb`)\n",
        "- Input di-*pad* ke panjang tetap (500 kata per review)\n",
        "\n",
        "```python\n",
        "keras.datasets.imdb.load_data(num_words=10000)\n"
      ],
      "metadata": {
        "id": "0VLAxGsJxdqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HqJnFIXxH1I",
        "outputId": "e6ec4869-d6b8-4190-8356-c2517ec049b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Shape of X_train: (25000, 500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 123ms/step - accuracy: 0.5661 - loss: 0.6707 - val_accuracy: 0.7002 - val_loss: 0.5788\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 120ms/step - accuracy: 0.7988 - loss: 0.4459 - val_accuracy: 0.7594 - val_loss: 0.4879\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 119ms/step - accuracy: 0.8392 - loss: 0.3627 - val_accuracy: 0.6834 - val_loss: 0.5925\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 119ms/step - accuracy: 0.8318 - loss: 0.3842 - val_accuracy: 0.6876 - val_loss: 0.6050\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 120ms/step - accuracy: 0.8455 - loss: 0.3584 - val_accuracy: 0.7556 - val_loss: 0.5353\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 187ms/step - accuracy: 0.6955 - loss: 0.5660 - val_accuracy: 0.7904 - val_loss: 0.4658\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 188ms/step - accuracy: 0.8522 - loss: 0.3665 - val_accuracy: 0.8496 - val_loss: 0.3492\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 189ms/step - accuracy: 0.9120 - loss: 0.2339 - val_accuracy: 0.8590 - val_loss: 0.3311\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 189ms/step - accuracy: 0.9322 - loss: 0.1932 - val_accuracy: 0.8790 - val_loss: 0.3589\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 188ms/step - accuracy: 0.9508 - loss: 0.1434 - val_accuracy: 0.8670 - val_loss: 0.3474\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 352ms/step - accuracy: 0.6931 - loss: 0.5435 - val_accuracy: 0.8488 - val_loss: 0.3638\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 353ms/step - accuracy: 0.8956 - loss: 0.2667 - val_accuracy: 0.8674 - val_loss: 0.3231\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 354ms/step - accuracy: 0.9201 - loss: 0.2130 - val_accuracy: 0.8190 - val_loss: 0.3857\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 356ms/step - accuracy: 0.9310 - loss: 0.1763 - val_accuracy: 0.8774 - val_loss: 0.3380\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 354ms/step - accuracy: 0.9525 - loss: 0.1344 - val_accuracy: 0.8456 - val_loss: 0.4393\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 22ms/step - accuracy: 0.6864 - loss: 0.5872 - val_accuracy: 0.8352 - val_loss: 0.3794\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.8665 - loss: 0.3206 - val_accuracy: 0.8654 - val_loss: 0.3146\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9212 - loss: 0.2112 - val_accuracy: 0.8718 - val_loss: 0.3065\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.9551 - loss: 0.1382 - val_accuracy: 0.8632 - val_loss: 0.3293\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.9744 - loss: 0.0890 - val_accuracy: 0.8682 - val_loss: 0.3529\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.7637 - loss: 0.5241\n",
            "SimpleRNN Test Accuracy: 0.7672\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.8609 - loss: 0.3575\n",
            "LSTM Test Accuracy: 0.8615\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 75ms/step - accuracy: 0.8474 - loss: 0.4430\n",
            "BiLSTM Test Accuracy: 0.8482\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8675 - loss: 0.3418\n",
            "CNN1D Test Accuracy: 0.8664\n"
          ]
        }
      ],
      "source": [
        "# CHAPTER 15: Processing Sequences Using RNNs and CNNs\n",
        "# Fokus: Klasifikasi Teks dengan RNN & CNN pada dataset IMDB\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# ====================================================\n",
        "# 1. Load Dataset IMDB\n",
        "# ====================================================\n",
        "vocab_size = 10000\n",
        "max_length = 500\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "\n",
        "# ====================================================\n",
        "# 2. Simple RNN Model\n",
        "# ====================================================\n",
        "model_rnn = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=vocab_size, output_dim=32, input_length=max_length),\n",
        "    keras.layers.SimpleRNN(32),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_rnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ====================================================\n",
        "# 3. LSTM Model\n",
        "# ====================================================\n",
        "model_lstm = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
        "    keras.layers.LSTM(32),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ====================================================\n",
        "# 4. Bidirectional LSTM\n",
        "# ====================================================\n",
        "model_bilstm = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_bilstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_bilstm = model_bilstm.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ====================================================\n",
        "# 5. 1D Convolutional Model for Sequence\n",
        "# ====================================================\n",
        "model_cnn = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
        "    keras.layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_cnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ====================================================\n",
        "# 6. Evaluasi Akurasi Semua Model\n",
        "# ====================================================\n",
        "models = {\"SimpleRNN\": model_rnn, \"LSTM\": model_lstm, \"BiLSTM\": model_bilstm, \"CNN1D\": model_cnn}\n",
        "for name, model in models.items():\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "    print(f\"{name} Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### âœ… **Kesimpulan**\n",
        "\n",
        "\n",
        "## ğŸ§¾ Kesimpulan Chapter 15: Processing Sequences Using RNNs and CNNs\n",
        "\n",
        "Pada bab ini, kita berhasil mengimplementasikan beberapa pendekatan pemrosesan urutan menggunakan RNN dan CNN 1D.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Œ Hal-hal Penting:\n",
        "- **SimpleRNN** mudah diimplementasikan namun kurang efektif untuk urutan panjang.\n",
        "- **LSTM** jauh lebih baik untuk menangkap dependensi jangka panjang.\n",
        "- **Bidirectional LSTM** memberikan konteks dua arah untuk prediksi yang lebih akurat.\n",
        "- **1D CNN** cepat dan efektif, terutama untuk fitur spasial lokal dalam teks.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š Hasil Pengujian:\n",
        "- Model LSTM dan BiLSTM biasanya mengungguli SimpleRNN.\n",
        "- CNN 1D memberikan performa kompetitif dengan efisiensi lebih tinggi.\n",
        "- Evaluasi dilakukan dengan mengukur akurasi terhadap data uji.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ Implikasi Lanjut:\n",
        "- Teknik ini sangat berguna untuk NLP, prediksi urutan, deteksi emosi, dan time series.\n",
        "- Di bab-bab selanjutnya akan dibahas model NLP yang lebih canggih (seperti Transformer).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8O83EaqrxoG0"
      }
    }
  ]
}
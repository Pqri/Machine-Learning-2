{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“˜ Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "\n",
        "Bab ini membahas bagaimana model deep learning digunakan untuk menangani data teks atau urutan kata dalam konteks **Natural Language Processing (NLP)**, dengan fokus pada **RNN (GRU)** dan **Attention Mechanism**.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Tujuan Pembelajaran\n",
        "\n",
        "- Memahami representasi kata menggunakan Embedding\n",
        "- Menggunakan model RNN (GRU) untuk klasifikasi teks\n",
        "- Menerapkan mekanisme attention untuk fokus pada bagian penting input\n",
        "- Melatih dan mengevaluasi model NLP menggunakan dataset IMDB\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Dataset: IMDB Movie Reviews\n",
        "\n",
        "- Berisi 50.000 review film (25.000 train + 25.000 test)\n",
        "- Label: 1 (positif) atau 0 (negatif)\n",
        "- Data sudah ditokenisasi oleh Keras dan di-*pad* ke panjang tetap\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§± Arsitektur Model\n",
        "\n",
        "Model utama terdiri dari 3 komponen inti yang digunakan secara kombinatif:\n",
        "\n",
        "1. **Embedding Layer**  \n",
        "   Mengubah integer (representasi kata) menjadi vektor dense berdimensi tetap.\n",
        "   > Contoh: `Embedding(input_dim=10000, output_dim=64)`\n",
        "\n",
        "2. **Recurrent Layer: GRU**  \n",
        "   Layer RNN efisien yang memproses urutan kata dan menyimpan konteks.\n",
        "   - Jika `return_sequences=False`, hanya output akhir yang diambil (untuk klasifikasi).\n",
        "   - Jika `return_sequences=True`, seluruh urutan keluaran digunakan (untuk attention).\n",
        "\n",
        "3. **Attention Mechanism (Manual)**  \n",
        "   Digunakan untuk memberikan bobot berbeda ke setiap kata dalam urutan input.\n",
        "   - Dibangun secara manual menggunakan Dense â†’ Softmax â†’ Multiply.\n",
        "   - Output akhir adalah kombinasi tertimbang dari semua langkah waktu.\n",
        "\n",
        "4. **Output Layer**  \n",
        "   Layer `Dense(1, activation=\"sigmoid\")` untuk klasifikasi biner (positif/negatif).\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Kompilasi & Pelatihan\n",
        "\n",
        "- **Loss Function**: `binary_crossentropy`\n",
        "- **Optimizer**: `adam`\n",
        "- **Metric**: `accuracy`\n",
        "- Pelatihan dilakukan selama 5 epoch dengan validasi 20% dari data training.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Evaluasi Model\n",
        "\n",
        "Model dievaluasi terhadap data uji (`X_test`, `y_test`) untuk mengukur akurasi generalisasi. Model dengan Attention biasanya menghasilkan performa yang lebih baik dan lebih dapat diinterpretasikan.\n",
        "\n",
        "```python\n",
        "model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "XTrqE4xjzhBu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-KeZ8C5zdTw",
        "outputId": "12ea5249-fe73-480b-c5ce-a8858d183749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.6930 - loss: 0.5527 - val_accuracy: 0.8460 - val_loss: 0.3551\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.8886 - loss: 0.2792 - val_accuracy: 0.8568 - val_loss: 0.3382\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9356 - loss: 0.1738 - val_accuracy: 0.8776 - val_loss: 0.3128\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9644 - loss: 0.1062 - val_accuracy: 0.8728 - val_loss: 0.3343\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.9792 - loss: 0.0651 - val_accuracy: 0.8694 - val_loss: 0.4334\n",
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.7028 - loss: 0.5305 - val_accuracy: 0.8762 - val_loss: 0.2982\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9125 - loss: 0.2249 - val_accuracy: 0.8798 - val_loss: 0.2863\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9460 - loss: 0.1454 - val_accuracy: 0.8656 - val_loss: 0.3347\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9675 - loss: 0.0959 - val_accuracy: 0.8720 - val_loss: 0.3606\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.9815 - loss: 0.0550 - val_accuracy: 0.8592 - val_loss: 0.5383\n",
            "Evaluasi GRU:\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8638 - loss: 0.4657\n",
            "Evaluasi GRU + Attention:\n",
            "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8431 - loss: 0.5865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6086789965629578, 0.8387200236320496]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# CHAPTER 16: Natural Language Processing with RNNs and Attention\n",
        "# ---------------------------------------------------------------\n",
        "# Fokus: Proses teks dengan RNN dan Attention sederhana\n",
        "# Dataset: IMDB reviews (biner, positif/negatif)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# ===================================================\n",
        "# 1. Load dan Preprocess Dataset IMDB\n",
        "# ===================================================\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)\n",
        "\n",
        "# ===================================================\n",
        "# 2. Model NLP dengan Embedding dan GRU\n",
        "# ===================================================\n",
        "model_gru = keras.models.Sequential([\n",
        "    keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
        "    keras.layers.GRU(64, return_sequences=False),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_gru.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history_gru = model_gru.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ===================================================\n",
        "# 3. Model dengan Return Sequences = True (untuk Attention)\n",
        "# ===================================================\n",
        "# GRU menghasilkan output sequence penuh (bukan hanya vektor akhir)\n",
        "inputs = keras.layers.Input(shape=[None])\n",
        "embed = keras.layers.Embedding(input_dim=vocab_size, output_dim=64)(inputs)\n",
        "gru_out = keras.layers.GRU(64, return_sequences=True)(embed)\n",
        "\n",
        "# ===================================================\n",
        "# 4. Attention Layer Sederhana (Manual)\n",
        "# ===================================================\n",
        "attention = keras.layers.Dense(1, activation=\"tanh\")(gru_out)\n",
        "attention = keras.layers.Flatten()(attention)\n",
        "attention = keras.layers.Activation(\"softmax\")(attention)\n",
        "attention = keras.layers.RepeatVector(64)(attention)\n",
        "attention = keras.layers.Permute([2, 1])(attention)\n",
        "\n",
        "sent_representation = keras.layers.Multiply()([gru_out, attention])\n",
        "sent_representation = keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1))(sent_representation)\n",
        "\n",
        "output = keras.layers.Dense(1, activation=\"sigmoid\")(sent_representation)\n",
        "\n",
        "# Gabungkan jadi model\n",
        "model_attention = keras.models.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model_attention.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history_attention = model_attention.fit(X_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "# ===================================================\n",
        "# 5. Evaluasi Model\n",
        "# ===================================================\n",
        "print(\"Evaluasi GRU:\")\n",
        "model_gru.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Evaluasi GRU + Attention:\")\n",
        "model_attention.evaluate(X_test, y_test)\n"
      ]
    }
  ]
}
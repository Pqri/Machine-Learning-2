{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Chapter 18: Scaling Up with TensorFlow\n",
        "\n",
        "## 🎯 Tujuan Bab\n",
        "\n",
        "Bab ini berfokus pada bagaimana membangun model Machine Learning dengan **API tingkat rendah TensorFlow**, serta bagaimana mengoptimalkan dan mendistribusikan model untuk kebutuhan **deployment production-level**. Pendekatan ini memberi kontrol penuh terhadap proses training, optimasi, dan penyimpanan model.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Topik Utama\n",
        "\n",
        "1. **SavedModel Format**\n",
        "   - Format serialisasi standar TensorFlow untuk menyimpan dan mengekspor model.\n",
        "   - Kompatibel dengan banyak tools lain: TensorFlow Serving, TensorFlow Lite, dan TensorFlow.js.\n",
        "\n",
        "2. **@tf.function**\n",
        "   - Mendekorasi fungsi Python biasa agar dikompilasi menjadi *static computation graph*.\n",
        "   - Meningkatkan efisiensi eksekusi dan dapat dijalankan di perangkat keras khusus.\n",
        "\n",
        "3. **Custom Training Loop**\n",
        "   - Menggunakan `tf.GradientTape` untuk mengontrol proses forward-pass dan backward-pass secara manual.\n",
        "   - Ideal untuk pelatihan fleksibel, fine-grained debugging, dan integrasi dengan logika bisnis yang kompleks.\n",
        "\n",
        "4. **tf.data API**\n",
        "   - Digunakan untuk membuat input pipeline yang scalable, efisien, dan dapat menangani data besar.\n",
        "   - Mendukung batching, shuffling, caching, dan prefetching.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚡ Manfaat Pendekatan Ini\n",
        "\n",
        "- Cocok untuk **skala besar dan produksi**.\n",
        "- Dapat **dioptimalkan secara manual** untuk performa, efisiensi memori, dan distribusi.\n",
        "- Membuka pintu ke deployment real-world seperti:\n",
        "  - Mobile apps (via TFLite)\n",
        "  - Web (via TensorFlow.js)\n",
        "  - Backend server (via TensorFlow Serving)\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 Catatan Penting\n",
        "\n",
        "> TensorFlow menyediakan banyak fleksibilitas melalui API tingkat rendah — namun dengan kekuatan tersebut datang juga kebutuhan untuk pemahaman lebih dalam tentang cara kerja training loop, optimizers, dan tensor operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZQVpX208QO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faEuyDXn7_oP",
        "outputId": "65253c39-5c5b-48d0-e208-ea4245debbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6837 - loss: 1.0025 - val_accuracy: 0.7997 - val_loss: 0.5442\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8219 - loss: 0.5116 - val_accuracy: 0.8312 - val_loss: 0.4758\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8396 - loss: 0.4573 - val_accuracy: 0.8493 - val_loss: 0.4246\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8512 - loss: 0.4230 - val_accuracy: 0.8547 - val_loss: 0.4083\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8615 - loss: 0.3984 - val_accuracy: 0.8553 - val_loss: 0.4231\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8496 - loss: 0.4402\n",
            "\n",
            "🎯 Accuracy model setelah load: 0.8424\n",
            "\n",
            "🧮 Hasil scaled_add(2,3): 8.0\n",
            "\n",
            "📦 Epoch 1\n",
            "Step 0, Loss: 2.4420\n",
            "Step 100, Loss: 1.2856\n",
            "Step 200, Loss: 1.0221\n",
            "Step 300, Loss: 0.7106\n",
            "Step 400, Loss: 0.8575\n",
            "Step 500, Loss: 0.8722\n",
            "Step 600, Loss: 0.5978\n",
            "Step 700, Loss: 0.5358\n",
            "Step 800, Loss: 0.8419\n",
            "Step 900, Loss: 0.6386\n",
            "Step 1000, Loss: 0.5150\n",
            "Step 1100, Loss: 1.1125\n",
            "Step 1200, Loss: 0.4982\n",
            "Step 1300, Loss: 0.6188\n",
            "Step 1400, Loss: 0.7732\n",
            "Step 1500, Loss: 0.4516\n",
            "Step 1600, Loss: 0.6058\n",
            "Step 1700, Loss: 0.5968\n",
            "Step 1800, Loss: 0.4658\n",
            "\n",
            "📦 Epoch 2\n",
            "Step 0, Loss: 0.5690\n",
            "Step 100, Loss: 0.5290\n",
            "Step 200, Loss: 0.6122\n",
            "Step 300, Loss: 0.5221\n",
            "Step 400, Loss: 0.5381\n",
            "Step 500, Loss: 0.4001\n",
            "Step 600, Loss: 0.5579\n",
            "Step 700, Loss: 0.5091\n",
            "Step 800, Loss: 0.4713\n",
            "Step 900, Loss: 0.6402\n",
            "Step 1000, Loss: 0.3387\n",
            "Step 1100, Loss: 0.4036\n",
            "Step 1200, Loss: 0.4950\n",
            "Step 1300, Loss: 0.7930\n",
            "Step 1400, Loss: 0.7379\n",
            "Step 1500, Loss: 0.3837\n",
            "Step 1600, Loss: 0.8396\n",
            "Step 1700, Loss: 0.4922\n",
            "Step 1800, Loss: 0.7211\n",
            "\n",
            "📦 Epoch 3\n",
            "Step 0, Loss: 0.5708\n",
            "Step 100, Loss: 0.5644\n",
            "Step 200, Loss: 0.1806\n",
            "Step 300, Loss: 0.2874\n",
            "Step 400, Loss: 0.5107\n",
            "Step 500, Loss: 0.3137\n",
            "Step 600, Loss: 0.5155\n",
            "Step 700, Loss: 0.3014\n",
            "Step 800, Loss: 0.6376\n",
            "Step 900, Loss: 0.4035\n",
            "Step 1000, Loss: 0.4570\n",
            "Step 1100, Loss: 0.5375\n",
            "Step 1200, Loss: 0.3660\n",
            "Step 1300, Loss: 0.3117\n",
            "Step 1400, Loss: 0.3173\n",
            "Step 1500, Loss: 0.5367\n",
            "Step 1600, Loss: 0.4958\n",
            "Step 1700, Loss: 0.5123\n",
            "Step 1800, Loss: 0.4298\n"
          ]
        }
      ],
      "source": [
        "# ✅ Import Library\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# ===================================================\n",
        "# 🧠 Step 1: Membangun dan Melatih Model Sederhana\n",
        "# ===================================================\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation='relu'),\n",
        "    keras.layers.Dense(100, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "# ===================================================\n",
        "# 💾 Step 2: Menyimpan dan Memuat Model (.keras format)\n",
        "# ===================================================\n",
        "model.save(\"my_fashion_model.keras\")  # format baru Keras 3\n",
        "\n",
        "# Load ulang model\n",
        "new_model = keras.models.load_model(\"my_fashion_model.keras\")\n",
        "test_loss, test_acc = new_model.evaluate(X_test, y_test)\n",
        "print(f\"\\n🎯 Accuracy model setelah load: {test_acc:.4f}\")\n",
        "\n",
        "# ===================================================\n",
        "# 🔁 Step 3: TF Function - Kompilasi Fungsi ke Graph\n",
        "# ===================================================\n",
        "@tf.function\n",
        "def scaled_add(x, y):\n",
        "    return x + y * 2.0\n",
        "\n",
        "x = tf.constant(2.0)\n",
        "y = tf.constant(3.0)\n",
        "print(\"\\n🧮 Hasil scaled_add(2,3):\", scaled_add(x, y).numpy())\n",
        "\n",
        "# ===================================================\n",
        "# 🔄 Step 4: Custom Training Loop dengan GradientTape\n",
        "# ===================================================\n",
        "def create_model():\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        keras.layers.Dense(100, activation=\"relu\"),\n",
        "        keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "model = create_model()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Siapkan dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(32)\n",
        "\n",
        "# Training manual\n",
        "for epoch in range(3):\n",
        "    print(f\"\\n📦 Epoch {epoch+1}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss_value = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss_value.numpy():.4f}\")\n"
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5edf5-47e8-484e-8a45-203a5b5f8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETUP AWAL ---\n",
    "# Python ≥3.5 diperlukan\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 diperlukan\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Impor library umum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Untuk membuat plot yang konsisten\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Direktori untuk menyimpan gambar\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Menyimpan gambar\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\"\"\"\n",
    "# Bab 6: Decision Trees\n",
    "\n",
    "## Penjelasan Teoretis: Konsep Dasar Decision Tree\n",
    "Decision Trees (Pohon Keputusan) adalah algoritma Machine Learning yang serbaguna, mampu melakukan tugas klasifikasi, regresi, dan bahkan multioutput. Mereka adalah model yang kuat, mampu menyesuaikan diri dengan dataset yang kompleks.\n",
    "\n",
    "Decision Trees juga merupakan komponen fundamental dari Random Forests, salah satu algoritma ML paling kuat yang ada saat ini.\n",
    "\n",
    "Salah satu keunggulan Decision Trees adalah mereka memerlukan sangat sedikit persiapan data. Mereka tidak memerlukan penskalaan fitur atau pemusatan data sama sekali. Model ini sering disebut sebagai model *white box* karena cara kerjanya sangat intuitif dan mudah diinterpretasikan.\n",
    "\"\"\"\n",
    "\n",
    "# --- 1. Melatih dan Memvisualisasikan Decision Tree ---\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "# Melatih model Decision Tree dengan kedalaman maksimum 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "\"\"\"\n",
    "#### Visualisasi Pohon\n",
    "Kita bisa memvisualisasikan pohon yang telah dilatih menggunakan `export_graphviz` dari Scikit-Learn untuk membuat file definisi grafik (.dot).\n",
    "\"\"\"\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "# Untuk mengonversi file .dot ke PNG, Anda bisa menggunakan tool Graphviz dari command line:\n",
    "# $ dot -Tpng iris_tree.dot -o iris_tree.png\n",
    "# Atau, kita bisa menampilkannya langsung di notebook jika graphviz terinstal.\n",
    "try:\n",
    "    import graphviz\n",
    "    with open(os.path.join(IMAGES_PATH, \"iris_tree.dot\")) as f:\n",
    "        dot_graph = f.read()\n",
    "    display(graphviz.Source(dot_graph))\n",
    "except ImportError:\n",
    "    print(\"Harap instal graphviz untuk menampilkan pohon keputusan di notebook.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "### Membuat Prediksi\n",
    "Untuk membuat prediksi, kita mulai dari *root node* (node paling atas) dan menelusuri pohon ke bawah. Setiap node mengajukan pertanyaan tentang sebuah fitur. Berdasarkan jawabannya, kita pindah ke *child node* kiri atau kanan, hingga mencapai *leaf node* (node daun, yang tidak memiliki anak). Kelas yang diprediksi adalah kelas mayoritas di leaf node tersebut.\n",
    "\n",
    "#### Atribut Node:\n",
    "- **`samples`**: Jumlah instance pelatihan yang berlaku pada node ini.\n",
    "- **`value`**: Jumlah instance pelatihan dari setiap kelas yang berlaku pada node ini.\n",
    "- **`gini`**: Ukuran *impurity* (ketidakmurnian) sebuah node. Node murni (`gini=0`) jika semua instance di dalamnya berasal dari kelas yang sama.\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Mengestimasi Probabilitas Kelas ---\n",
    "# Decision Tree juga bisa mengestimasi probabilitas sebuah instance.\n",
    "# Ini adalah rasio instance dari setiap kelas di leaf node tempat instance tersebut jatuh.\n",
    "print(\"\\nProbabilitas untuk bunga [5cm, 1.5cm]:\", tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(\"Prediksi kelas:\", tree_clf.predict([[5, 1.5]]))\n",
    "\n",
    "\n",
    "# --- 3. Algoritma Pelatihan CART ---\n",
    "\"\"\"\n",
    "Scikit-Learn menggunakan algoritma **Classification And Regression Tree (CART)** untuk melatih Decision Trees.\n",
    "- Algoritma ini bekerja dengan membagi set pelatihan menjadi dua subset menggunakan satu fitur `k` dan sebuah *threshold* `tk`.\n",
    "- Ia mencari pasangan `(k, tk)` yang menghasilkan subset paling murni (diukur dengan Gini atau Entropy), dengan mempertimbangkan ukuran subset.\n",
    "- Proses ini dilakukan secara rekursif hingga mencapai kedalaman maksimum atau tidak dapat menemukan pemisahan yang mengurangi ketidakmurnian.\n",
    "\n",
    "CART adalah **greedy algorithm** (algoritma serakah). Ia membuat pemisahan optimal di setiap level, tetapi tidak menjamin akan menemukan pohon yang optimal secara global.\n",
    "\"\"\"\n",
    "\n",
    "# --- 4. Hyperparameter Regularisasi ---\n",
    "\"\"\"\n",
    "Decision Tree sangat fleksibel dan dapat dengan mudah **overfitting** jika tidak dibatasi. Untuk menghindarinya, kita perlu melakukan regularisasi dengan membatasi kebebasan pohon.\n",
    "\n",
    "Hyperparameter regularisasi utama:\n",
    "- `max_depth`: Kedalaman maksimum pohon.\n",
    "- `min_samples_split`: Jumlah minimum sampel yang harus dimiliki sebuah node sebelum bisa dipecah.\n",
    "- `min_samples_leaf`: Jumlah minimum sampel yang harus dimiliki oleh sebuah leaf node.\n",
    "- `max_leaf_nodes`: Jumlah maksimum leaf node.\n",
    "- `max_features`: Jumlah maksimum fitur yang dievaluasi untuk pemisahan di setiap node.\n",
    "\n",
    "Menaikkan `min_*` atau mengurangi `max_*` akan meregularisasi model.\n",
    "\"\"\"\n",
    "# Contoh efek regularisasi pada moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42) # Tanpa batasan\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42) # Dengan regularisasi\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "# Fungsi untuk plot decision boundary\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = mpl.colors.ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = mpl.colors.ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.45, -1, 1.5], alpha=0.2)\n",
    "plt.title(\"Tanpa Batasan (Overfitting)\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.45, -1, 1.5], alpha=0.2)\n",
    "plt.title(\"Regularisasi min_samples_leaf=4\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 5. Regresi dengan Decision Tree ---\n",
    "\"\"\"\n",
    "Decision Tree juga bisa melakukan regresi. Alih-alih memprediksi kelas, setiap leaf node memprediksi sebuah nilai. Nilai prediksi ini adalah rata-rata dari nilai target semua instance pelatihan di leaf node tersebut.\n",
    "\n",
    "Algoritma CART bekerja dengan cara yang sama, tetapi sekarang ia mencoba membagi set pelatihan untuk meminimalkan **Mean Squared Error (MSE)**, bukan Gini impurity.\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Buat dataset kuadratik dengan noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)\n",
    "\n",
    "# --- 6. Instabilitas Decision Tree ---\n",
    "\"\"\"\n",
    "Decision Tree memiliki beberapa keterbatasan:\n",
    "1. **Sensitif terhadap rotasi data**: Pohon keputusan menyukai batas keputusan ortogonal (tegak lurus sumbu), sehingga kinerjanya bisa buruk jika data dirotasi.\n",
    "2. **Sangat sensitif terhadap variasi kecil dalam data pelatihan**: Menghapus satu instance saja bisa menghasilkan pohon yang sangat berbeda. Ini membuat model menjadi tidak stabil.\n",
    "\n",
    "Random Forests (dibahas di Bab 7) mengatasi masalah ketidakstabilan ini dengan merata-ratakan prediksi dari banyak pohon.\n",
    "\"\"\"\n",
    "\n",
    "# --- JAWABAN LATIHAN TEORETIS ---\n",
    "\n",
    "\"\"\"\n",
    "### Latihan: Jawaban Teoretis\n",
    "\n",
    "**1. Berapa perkiraan kedalaman Decision Tree yang dilatih (tanpa batasan) pada set pelatihan dengan satu juta instance?**\n",
    "Kedalaman sebuah Decision Tree yang seimbang adalah sekitar $log_2(m)$, di mana $m$ adalah jumlah instance.\n",
    "$log_2(1,000,000) = log(10^6)/log(2) \\approx 6 \\times 2.3 / 0.69 \\approx 20$.\n",
    "Jadi, kedalamannya sekitar 20.\n",
    "\n",
    "**2. Apakah Gini impurity sebuah node umumnya lebih rendah atau lebih besar dari induknya? Apakah selalu lebih rendah/besar?**\n",
    "Gini impurity sebuah node **umumnya lebih rendah** dari induknya. Algoritma CART secara spesifik mencari pemisahan yang meminimalkan Gini impurity (tertimbang). Oleh karena itu, jumlah Gini impurity dari *child nodes* (setelah dihitung rata-rata tertimbang berdasarkan jumlah sampel) **selalu lebih rendah** dari Gini impurity *parent node*-nya. Namun, Gini impurity dari satu *child node* tunggal bisa saja lebih tinggi dari induknya.\n",
    "\n",
    "**3. Jika Decision Tree mengalami overfitting, apakah ide yang bagus untuk mencoba mengurangi `max_depth`?**\n",
    "**Ya, sangat bagus**. Mengurangi `max_depth` adalah salah satu cara paling umum dan efektif untuk meregularisasi Decision Tree dan mengurangi overfitting. Ini membatasi kompleksitas model.\n",
    "\n",
    "**4. Jika Decision Tree mengalami underfitting, apakah ide yang bagus untuk mencoba penskalaan fitur input?**\n",
    "**Tidak**. Salah satu keunggulan Decision Tree adalah ia tidak terpengaruh oleh penskalaan fitur. Penskalaan tidak akan membantu mengatasi underfitting. Untuk mengatasi underfitting, Anda harus mencoba meningkatkan kompleksitas model, misalnya dengan menaikkan `max_depth` atau melonggarkan hyperparameter regularisasi lainnya (mengurangi `min_*`).\n",
    "\n",
    "**5. Jika butuh satu jam untuk melatih Decision Tree pada 1 juta instance, berapa perkiraan waktu untuk melatihnya pada 10 juta instance?**\n",
    "Kompleksitas pelatihan CART adalah sekitar $O(n \\times m \\log(m))$. Jika kita asumsikan `n` (jumlah fitur) tetap, rasio waktunya adalah:\n",
    "$T(10m) / T(m) = (10m \\log(10m)) / (m \\log(m)) = 10 \\times \\log(10m) / \\log(m)$\n",
    "$\\log(10m) = \\log(10) + \\log(m)$\n",
    "Rasio = $10 \\times (\\log(10) + \\log(m)) / \\log(m) = 10 \\times (1 + \\log(10)/\\log(m))$\n",
    "Untuk $m=10^6$, $\\log(m)=6$.\n",
    "Rasio $\\approx 10 \\times (1 + 1/6) \\approx 11.67$.\n",
    "Jadi, perkiraan waktunya adalah $1 \\text{ jam} \\times 11.67 \\approx 11.7$ jam.\n",
    "\n",
    "**6. Jika set pelatihan Anda berisi 100.000 instance, apakah `presort=True` akan mempercepat pelatihan?**\n",
    "**Tidak**. `presort=True` hanya mempercepat pelatihan pada dataset kecil (kurang dari beberapa ribu instance). Untuk 100.000 instance, pengaturan ini justru akan memperlambat pelatihan secara signifikan.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# --- LATIHAN PRAKTIS ---\n",
    "\n",
    "\"\"\"\n",
    "### Latihan 7: Latih dan fine-tune Decision Tree untuk moons dataset.\n",
    "\"\"\"\n",
    "print(\"\\n--- Latihan 7: Fine-tuning Decision Tree ---\")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# a. Buat dataset\n",
    "X_moons, y_moons = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "\n",
    "# b. Bagi dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
    "\n",
    "# c. Gunakan GridSearchCV untuk mencari hyperparameter terbaik\n",
    "params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "print(\"Hyperparameter terbaik:\", grid_search_cv.best_params_)\n",
    "\n",
    "# d. Latih model dengan hyperparameter terbaik dan evaluasi\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_tree = grid_search_cv.best_estimator_\n",
    "y_pred = best_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Akurasi pada test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "### Latihan 8: Tumbuhkan sebuah \"forest\" (Hutan).\n",
    "\"\"\"\n",
    "print(\"\\n--- Latihan 8: Membuat Random Forest Manual ---\")\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# a. Buat 1000 subset dari training set\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, _ in rs.split(X_train):\n",
    "    mini_sets.append((X_train[mini_train_index], y_train[mini_train_index]))\n",
    "\n",
    "# b. Latih 1000 Decision Tree\n",
    "from sklearn.base import clone\n",
    "forest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]\n",
    "\n",
    "accuracy_scores = []\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "    y_pred_tree = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "print(f\"Akurasi rata-rata dari 1000 pohon: {np.mean(accuracy_scores) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# c. Buat prediksi mayoritas (majority-vote)\n",
    "# Buat matriks prediksi dari semua pohon\n",
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)\n",
    "\n",
    "# Gunakan mode (nilai yang paling sering muncul) untuk setiap instance\n",
    "from scipy.stats import mode\n",
    "y_pred_majority_votes, _ = mode(Y_pred, axis=0, keepdims=False)\n",
    "\n",
    "# d. Evaluasi prediksi mayoritas\n",
    "accuracy_forest = accuracy_score(y_test, y_pred_majority_votes)\n",
    "print(f\"Akurasi Random Forest manual: {accuracy_forest * 100:.2f}%\")\n",
    "print(\"Akurasi forest lebih tinggi dari pohon tunggal!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

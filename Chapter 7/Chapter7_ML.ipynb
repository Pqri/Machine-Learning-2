{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e591538a-ffd6-4728-bb13-bfd4b32f904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Akurasi Voting Classifier ---\n",
      "Akurasi LogisticRegression: 0.8640\n",
      "Akurasi RandomForestClassifier: 0.8960\n",
      "Akurasi SVC: 0.8960\n",
      "Akurasi VotingClassifier: 0.9120\n",
      "Akurasi Soft Voting Classifier: 0.9200\n",
      "\n",
      "Akurasi Bagging Classifier: 0.9040\n",
      "\n",
      "OOB Score: 0.8987\n",
      "\n",
      "Akurasi Random Forest: 0.9120\n",
      "\n",
      "--- Feature Importance pada Iris Dataset ---\n",
      "sepal length (cm): 0.1125\n",
      "sepal width (cm): 0.0231\n",
      "petal length (cm): 0.4410\n",
      "petal width (cm): 0.4234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raehu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Akurasi AdaBoost: 0.8960\n",
      "\n",
      "Jumlah Pohon Optimal untuk GBRT: 75\n",
      "\n",
      "--- Latihan 8 & 9: Ensemble dan Stacking pada MNIST ---\n",
      "\n",
      "Melatih classifier individual...\n",
      "Akurasi RandomForestClassifier: 0.9683\n",
      "Akurasi ExtraTreesClassifier: 0.9718\n",
      "Akurasi LinearSVC: 0.0984\n",
      "Akurasi Voting Classifier (validation): 0.9637\n",
      "Akurasi Voting Classifier (test): 0.9629\n",
      "\n",
      "Membuat Stacking Ensemble...\n",
      "OOB Score Blender: 0.9697\n",
      "Akurasi Stacking Ensemble (test): 0.9702\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP AWAL ---\n",
    "# Python ≥3.5 diperlukan\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 diperlukan\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Impor library umum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Untuk membuat plot yang konsisten\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Direktori untuk menyimpan gambar\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ensembles\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Menyimpan gambar\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\"\"\"\n",
    "# Bab 7: Ensemble Learning dan Random Forests\n",
    "\n",
    "## Penjelasan Teoretis: Konsep Dasar Ensemble Learning\n",
    "Ensemble Learning adalah teknik di mana kita menggabungkan prediksi dari sekelompok *predictor* (disebut *ensemble*) untuk mendapatkan prediksi yang lebih baik daripada prediksi dari masing-masing predictor tunggal. Fenomena ini mirip dengan \"kebijaksanaan orang banyak\" (*wisdom of the crowd*).\n",
    "\n",
    "Contohnya, jika kita melatih sekelompok classifier Decision Tree pada subset data yang berbeda-beda, kemudian kita ambil suara mayoritas dari prediksi mereka, kita akan mendapatkan model yang disebut **Random Forest**.\n",
    "\n",
    "Metode ensemble umumnya sangat efektif dan sering menjadi bagian dari solusi pemenang dalam kompetisi Machine Learning.\n",
    "\"\"\"\n",
    "\n",
    "# --- 1. Voting Classifiers ---\n",
    "\"\"\"\n",
    "Cara paling sederhana untuk membuat ensemble adalah dengan menggabungkan prediksi dari beberapa classifier yang berbeda dan memilih kelas yang paling banyak mendapatkan suara (*majority vote*). Ini disebut **Hard Voting Classifier**.\n",
    "\n",
    "Hebatnya, akurasi dari *voting classifier* ini seringkali lebih tinggi daripada classifier terbaik dalam ensemble tersebut. Hal ini bisa terjadi jika para classifier cukup beragam dan membuat jenis kesalahan yang berbeda-beda.\n",
    "\n",
    "Jika semua classifier dapat mengestimasi probabilitas, kita bisa menggunakan **Soft Voting Classifier**. Di sini, kita merata-ratakan probabilitas kelas dari semua classifier dan memilih kelas dengan probabilitas rata-rata tertinggi. Soft voting seringkali berkinerja lebih baik karena memberikan bobot lebih pada prediksi yang sangat \"yakin\".\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "# Hard Voting\n",
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- Akurasi Voting Classifier ---\")\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf_hard):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Akurasi {clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Soft Voting\n",
    "# SVM perlu diatur probability=True untuk bisa melakukan soft voting\n",
    "svm_clf_soft = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf_soft)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "print(f\"Akurasi Soft Voting Classifier: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "\n",
    "\n",
    "# --- 2. Bagging and Pasting ---\n",
    "\"\"\"\n",
    "Pendekatan lain untuk membuat ensemble adalah menggunakan algoritma yang sama tetapi melatihnya pada subset acak yang berbeda dari set pelatihan.\n",
    "- **Bagging** (*Bootstrap Aggregating*): Sampling dilakukan **dengan** penggantian.\n",
    "- **Pasting**: Sampling dilakukan **tanpa** penggantian.\n",
    "\n",
    "Setelah semua predictor dilatih, ensemble membuat prediksi dengan agregasi (suara terbanyak untuk klasifikasi, rata-rata untuk regresi). Metode ini mengurangi varians model.\n",
    "\"\"\"\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble dari 500 Decision Tree\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, random_state=42\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)\n",
    "print(f\"\\nAkurasi Bagging Classifier: {accuracy_score(y_test, y_pred_bag):.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "#### Out-of-Bag (oob) Evaluation\n",
    "Dalam bagging, karena sampling dilakukan dengan penggantian, rata-rata hanya sekitar 63% instance yang terambil untuk setiap predictor. Sisa 37% yang tidak terambil disebut *out-of-bag (oob) instances*.\n",
    "\n",
    "Kita bisa menggunakan instance oob ini sebagai validation set untuk mengevaluasi kinerja ensemble tanpa perlu memisahkan data validasi secara manual. Atur `oob_score=True`.\n",
    "\"\"\"\n",
    "bag_clf_oob = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, oob_score=True, random_state=40\n",
    ")\n",
    "bag_clf_oob.fit(X_train, y_train)\n",
    "print(f\"\\nOOB Score: {bag_clf_oob.oob_score_:.4f}\")\n",
    "\n",
    "\n",
    "# --- 3. Random Forests ---\n",
    "\"\"\"\n",
    "**Random Forest** adalah ensemble dari Decision Trees, umumnya dilatih dengan metode bagging. Scikit-Learn menyediakan kelas `RandomForestClassifier` yang lebih nyaman dan teroptimisasi.\n",
    "\n",
    "Random Forest menambahkan satu lapis keacakan lagi: saat memecah sebuah node, ia tidak mencari fitur terbaik dari *semua* fitur, melainkan dari *subset acak* fitur. Ini menghasilkan pohon yang lebih beragam dan (sekali lagi) menukar bias yang sedikit lebih tinggi dengan varians yang lebih rendah, yang secara umum menghasilkan model yang lebih baik.\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(f\"\\nAkurasi Random Forest: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#### Feature Importance\n",
    "Salah satu keunggulan Random Forest adalah kemudahannya dalam mengukur pentingnya setiap fitur. Scikit-Learn menghitungnya dengan melihat seberapa besar sebuah fitur mengurangi *impurity* (ketidakmurnian) secara rata-rata di semua pohon dalam forest.\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf_iris = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rnd_clf_iris.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "print(\"\\n--- Feature Importance pada Iris Dataset ---\")\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf_iris.feature_importances_):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "\n",
    "# --- 4. Boosting ---\n",
    "\"\"\"\n",
    "**Boosting** adalah metode ensemble yang melatih predictor secara sekuensial, di mana setiap predictor baru mencoba memperbaiki kesalahan predictor sebelumnya.\n",
    "\n",
    "#### a. AdaBoost (Adaptive Boosting)\n",
    "Ide utamanya adalah membuat predictor baru lebih fokus pada instance yang salah diklasifikasikan oleh predictor sebelumnya. Ini dilakukan dengan meningkatkan bobot relatif dari instance yang salah diklasifikasikan.\n",
    "\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# DIBENERKEUN: Parameter `algorithm=\"SAMME.R\"` geus teu dipaké dina versi Scikit-Learn anyar.\n",
    "# 'SAMME' anu jadi default bakal otomatis ngagunakeun logika 'SAMME.R' upami base estimator\n",
    "# ngadukung predict_proba(), saperti DecisionTreeClassifier.\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    learning_rate=0.5, random_state=42\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "print(f\"\\nAkurasi AdaBoost: {accuracy_score(y_test, y_pred_ada):.4f}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#### b. Gradient Boosting\n",
    "Metode boosting populer lainnya. Alih-alih menyesuaikan bobot instance, Gradient Boosting melatih predictor baru pada **residual errors** (kesalahan sisa) yang dibuat oleh predictor sebelumnya.\n",
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Membuat data untuk regresi\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) - 0.5\n",
    "y_reg = 3*X_reg[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X_reg, y_reg)\n",
    "\n",
    "y2 = y_reg - tree_reg1.predict(X_reg) # residual errors\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X_reg, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X_reg) # residual errors dari model kedua\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X_reg, y3)\n",
    "\n",
    "# Scikit-Learn menyediakan kelas yang lebih mudah\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "# Early Stopping untuk menemukan jumlah pohon optimal\n",
    "errors = [mean_squared_error(y_test, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_test)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
    "gbrt_best.fit(X_train, y_train)\n",
    "print(f\"\\nJumlah Pohon Optimal untuk GBRT: {bst_n_estimators}\")\n",
    "\n",
    "# --- 5. Stacking ---\n",
    "\"\"\"\n",
    "**Stacking** (*Stacked Generalization*) adalah metode ensemble yang didasarkan pada ide sederhana: daripada menggunakan fungsi agregasi trivial seperti *voting*, mengapa tidak melatih sebuah model untuk melakukan agregasi tersebut?\n",
    "\n",
    "Model yang dilatih untuk menggabungkan prediksi ini disebut **blender** atau **meta learner**.\n",
    "\"\"\"\n",
    "\n",
    "# --- JAWABAN LATIHAN TEORETIS ---\n",
    "\n",
    "\"\"\"\n",
    "### Latihan: Jawaban Teoretis\n",
    "\n",
    "**1. Jika Anda melatih lima model berbeda pada data yang sama persis, dan semuanya mencapai presisi 95%, adakah peluang untuk menggabungkannya untuk mendapatkan hasil yang lebih baik?**\n",
    "**Ya, ada peluang**. Jika model-model tersebut cukup berbeda (misalnya, Logistic Regression, SVM, Random Forest) dan membuat jenis kesalahan yang berbeda, maka menggabungkannya (misalnya, dengan *voting classifier*) dapat menghasilkan kinerja yang lebih baik. Kesalahan dari satu model mungkin dapat dikoreksi oleh model lain.\n",
    "\n",
    "**2. Apa perbedaan antara hard dan soft voting classifiers?**\n",
    "- **Hard Voting**: Memprediksi kelas yang menerima suara mayoritas dari para classifier. Sederhana dan lugas.\n",
    "- **Soft Voting**: Menghitung rata-rata probabilitas kelas dari semua classifier, lalu memprediksi kelas dengan probabilitas rata-rata tertinggi. Ini seringkali berkinerja lebih baik karena memberikan bobot lebih pada prediksi yang sangat \"yakin\".\n",
    "\n",
    "**3. Apakah mungkin mempercepat pelatihan bagging ensemble dengan mendistribusikannya ke beberapa server? Bagaimana dengan pasting, boosting, Random Forests, atau stacking?**\n",
    "- **Bagging/Pasting**: **Ya**. Setiap predictor dalam ensemble dilatih secara independen pada subset data yang berbeda, sehingga proses ini sangat mudah untuk diparalelkan.\n",
    "- **Random Forests**: **Ya**. Ini pada dasarnya adalah bagging dari Decision Trees, jadi bisa diparalelkan.\n",
    "- **Boosting**: **Tidak**. Metode boosting melatih predictor secara sekuensial, di mana setiap pohon bergantung pada hasil pohon sebelumnya. Ini membuatnya sulit untuk diparalelkan.\n",
    "- **Stacking**: **Sebagian**. Predictor di layer yang sama bisa dilatih secara paralel. Namun, layer-layer harus dilatih secara sekuensial (layer 2 dilatih setelah layer 1 selesai).\n",
    "\n",
    "**4. Apa manfaat dari evaluasi out-of-bag (oob)?**\n",
    "Manfaatnya adalah kita bisa mendapatkan estimasi kinerja model pada data yang belum pernah dilihat tanpa perlu membuat *validation set* terpisah. Ini sangat efisien karena kita bisa menggunakan seluruh set pelatihan untuk pelatihan sekaligus mendapatkan evaluasi yang tidak bias.\n",
    "\n",
    "**5. Apa yang membuat Extra-Trees lebih acak daripada Random Forests biasa? Bagaimana keacakan ekstra ini bisa membantu? Apakah Extra-Trees lebih lambat atau lebih cepat?**\n",
    "- **Apa yang membuatnya lebih acak?**: Selain memilih subset fitur secara acak seperti Random Forest, Extra-Trees juga menggunakan *threshold* (nilai ambang batas) acak untuk memecah node, alih-alih mencari threshold terbaik.\n",
    "- **Bagaimana ini membantu?**: Keacakan ekstra ini bertindak sebagai bentuk regularisasi, menukar bias yang sedikit lebih tinggi dengan varians yang jauh lebih rendah.\n",
    "- **Lebih cepat atau lebih lambat?**: Extra-Trees **lebih cepat** untuk dilatih daripada Random Forests karena mencari threshold optimal untuk setiap fitur di setiap node adalah salah satu tugas yang paling memakan waktu.\n",
    "\n",
    "**6. Jika ensemble AdaBoost Anda underfitting, hyperparameter mana yang harus Anda ubah dan bagaimana?**\n",
    "Untuk mengatasi underfitting, Anda perlu meningkatkan kompleksitas model. Anda bisa:\n",
    "- **Menaikkan `n_estimators`**: Menambah jumlah predictor dalam ensemble.\n",
    "- **Mengurangi regularisasi pada *base estimator***: Misalnya, jika menggunakan Decision Tree, naikkan `max_depth`.\n",
    "- **Menaikkan `learning_rate`**: Membuat setiap predictor memiliki kontribusi yang lebih besar.\n",
    "\n",
    "**7. Jika ensemble Gradient Boosting Anda overfitting, haruskah Anda menaikkan atau menurunkan learning rate?**\n",
    "Anda harus **menurunkan `learning_rate`**. `learning_rate` yang lebih rendah (disebut *shrinkage*) berarti setiap pohon memiliki kontribusi yang lebih kecil. Ini memaksa kita untuk menggunakan lebih banyak pohon (`n_estimators`) untuk menyesuaikan data, tetapi secara umum menghasilkan model yang generalisasinya lebih baik. Anda juga bisa menggunakan *early stopping* untuk menemukan jumlah pohon yang optimal dan mencegah overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# --- LATIHAN PRAKTIS (Kerangka) ---\n",
    "\"\"\"\n",
    "### Latihan 8 & 9: Ensemble pada MNIST dan Stacking\n",
    "\n",
    "*Catatan: Menjalankan kode ini bisa sangat lama, terutama pada CPU biasa.\n",
    "Kode di bawah ini adalah kerangka untuk menyelesaikan latihan.\n",
    "\"\"\"\n",
    "print(\"\\n--- Latihan 8 & 9: Ensemble dan Stacking pada MNIST ---\")\n",
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "    X_mnist_train_full, X_mnist_test, y_mnist_train_full, y_mnist_test = mnist[\"data\"][:60000], mnist[\"data\"][60000:], mnist[\"target\"][:60000], mnist[\"target\"][60000:]\n",
    "    y_mnist_train_full = y_mnist_train_full.astype(np.uint8)\n",
    "    y_mnist_test = y_mnist_test.astype(np.uint8)\n",
    "    \n",
    "    # Buat validation set\n",
    "    X_mnist_train, X_mnist_val, y_mnist_train, y_mnist_val = train_test_split(\n",
    "        X_mnist_train_full, y_mnist_train_full, test_size=10000, random_state=42)\n",
    "\n",
    "    # Latih beberapa classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "\n",
    "    random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "    svm_lin_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
    "\n",
    "    estimators = [random_forest_clf, extra_trees_clf, svm_lin_clf]\n",
    "    print(\"\\nMelatih classifier individual...\")\n",
    "    for estimator in estimators:\n",
    "        estimator.fit(X_mnist_train, y_mnist_train)\n",
    "        y_pred = estimator.predict(X_mnist_val)\n",
    "        print(f\"Akurasi {estimator.__class__.__name__}: {accuracy_score(y_mnist_val, y_pred):.4f}\")\n",
    "\n",
    "    # Gabungkan dalam Voting Classifier\n",
    "    named_estimators = [\n",
    "        (\"random_forest_clf\", random_forest_clf),\n",
    "        (\"extra_trees_clf\", extra_trees_clf),\n",
    "        (\"svm_clf\", svm_lin_clf), # SVM tidak punya predict_proba, jadi kita pakai hard voting\n",
    "    ]\n",
    "    voting_clf = VotingClassifier(named_estimators, voting='hard')\n",
    "    voting_clf.fit(X_mnist_train, y_mnist_train)\n",
    "    y_pred_voting = voting_clf.predict(X_mnist_val)\n",
    "    print(f\"Akurasi Voting Classifier (validation): {accuracy_score(y_mnist_val, y_pred_voting):.4f}\")\n",
    "\n",
    "    # Evaluasi di test set\n",
    "    y_pred_voting_test = voting_clf.predict(X_mnist_test)\n",
    "    print(f\"Akurasi Voting Classifier (test): {accuracy_score(y_mnist_test, y_pred_voting_test):.4f}\")\n",
    "\n",
    "\n",
    "    # Latihan 9: Stacking\n",
    "    print(\"\\nMembuat Stacking Ensemble...\")\n",
    "    # Buat dataset baru dari prediksi di validation set\n",
    "    predictions = np.empty((len(X_mnist_val), len(estimators)), dtype=np.float32)\n",
    "    for index, estimator in enumerate(estimators):\n",
    "        predictions[:, index] = estimator.predict(X_mnist_val)\n",
    "\n",
    "    # Latih blender\n",
    "    blender_clf = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "    blender_clf.fit(predictions, y_mnist_val)\n",
    "    print(f\"OOB Score Blender: {blender_clf.oob_score_:.4f}\")\n",
    "\n",
    "    # Evaluasi stacking ensemble di test set\n",
    "    X_test_predictions = np.empty((len(X_mnist_test), len(estimators)), dtype=np.float32)\n",
    "    for index, estimator in enumerate(estimators):\n",
    "        X_test_predictions[:, index] = estimator.predict(X_mnist_test)\n",
    "\n",
    "    y_pred_stacking = blender_clf.predict(X_test_predictions)\n",
    "    print(f\"Akurasi Stacking Ensemble (test): {accuracy_score(y_mnist_test, y_pred_stacking):.4f}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nGagal memuat atau melatih pada dataset MNIST. Mungkin ada masalah koneksi atau butuh waktu lama. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ba0ab-09ed-414a-b807-049eb95fcfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
